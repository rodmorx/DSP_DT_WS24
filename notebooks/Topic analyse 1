!python -m spacy download de_core_news_md

import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from joblib import Parallel, delayed

# Sprachmodell laden
nlp = spacy.load("de_core_news_md")  # Größeres Modell für bessere Ergebnisse

# Funktion zur Extraktion von Nomen und Named Entities
def extract_key_terms(text):
    doc = nlp(text)
    nouns = [token.text.lower() for token in doc if token.pos_ == "NOUN"]
    entities = [ent.text.lower() for ent in doc.ents]
    return list(set(nouns + entities))  # Duplikate entfernen

# Funktion zur Themenfindung basierend auf TF-IDF
def get_topic_from_tfidf(documents):
    tfidf = TfidfVectorizer(max_features=1000, stop_words=None)
    X = tfidf.fit_transform(documents)
    feature_names = tfidf.get_feature_names_out()
    topics = []
    for row in X:
        top_indices = row.toarray()[0].argsort()[-5:][::-1]
        topics.append(", ".join([feature_names[i] for i in top_indices]))
    return topics

# Parallelisierte Extraktion von Schlüsselbegriffen
def parallel_extract_key_terms(texts):
    return Parallel(n_jobs=-1)(delayed(extract_key_terms)(text) for text in texts)



# Erstellen einer Kopie des Datasets
df = data
df_copy = df.copy()

# Sicherstellen, dass maintext kopierbar ist
df_copy.loc[:, "maintext"] = df_copy["maintext"].astype(str).copy()  # Kopieren, um sicherzustellen, dass die Daten beschreibbar sind

# Schlüsselbegriffe extrahieren ohne Parallelisierung
texts = df_copy["maintext"].tolist()  # Umwandlung in Liste
df_copy["key_terms"] = [extract_key_terms(text) for text in texts]  # Sequentiales Verarbeiten (ohne Parallelisierung)

# Key Terms zusammenführen
df_copy["key_terms_joined"] = df_copy["key_terms"].apply(lambda x: " ".join(x))

# TF-IDF-Themen erstellen
df_copy["topic"] = get_topic_from_tfidf(df_copy["key_terms_joined"])

# Ergebnisse speichern
df_copy.to_csv("out_with_topic.csv", index=False)
print("Datei wurde gespeichert als 'out_with_topic.csv'.")

!cp out_with_topic.csv /content/drive/MyDrive/
