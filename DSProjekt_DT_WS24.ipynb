{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodmorx/DSP_DT_WS24/blob/main/DSProjekt_DT_WS24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sience Projekt WS24 - Dream Team"
      ],
      "metadata": {
        "id": "4OrEuUXasy-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install news-please"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVYHF-4ts2yC",
        "outputId": "880fadf6-1d02-480f-b84c-7217f53d945b",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting news-please\n",
            "  Downloading news_please-1.6.13-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting Scrapy>=1.1.0 (from news-please)\n",
            "  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting PyMySQL>=0.7.9 (from news-please)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting psycopg2-binary>=2.8.4 (from news-please)\n",
            "  Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting hjson>=1.5.8 (from news-please)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting elasticsearch>=2.4 (from news-please)\n",
            "  Downloading elasticsearch-8.16.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.10/dist-packages (from news-please) (4.12.3)\n",
            "Collecting readability-lxml>=0.6.2 (from news-please)\n",
            "  Downloading readability_lxml-0.8.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langdetect>=1.0.7 (from news-please)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from news-please) (2.8.2)\n",
            "Collecting plac>=0.9.6 (from news-please)\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting dotmap>=1.2.17 (from news-please)\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting PyDispatcher>=2.0.5 (from news-please)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting warcio>=1.3.3 (from news-please)\n",
            "  Downloading warcio-1.7.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting ago>=0.0.9 (from news-please)\n",
            "  Downloading ago-0.0.95.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from news-please) (1.16.0)\n",
            "Requirement already satisfied: lxml>=3.3.5 in /usr/local/lib/python3.10/dist-packages (from news-please) (5.3.0)\n",
            "Collecting hurry.filesize>=0.9 (from news-please)\n",
            "  Downloading hurry.filesize-0.9.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4 (from news-please)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting faust-cchardet>=2.1.18 (from news-please)\n",
            "  Downloading faust_cchardet-2.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting boto3 (from news-please)\n",
            "  Downloading boto3-1.35.63-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting redis (from news-please)\n",
            "  Downloading redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting newspaper4k>=0.9.3.1 (from news-please)\n",
            "  Downloading newspaper4k-0.9.3.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting lxml-html-clean>=0.1.1 (from news-please)\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from news-please) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.3.2->news-please) (2.6)\n",
            "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch>=2.4->news-please)\n",
            "  Downloading elastic_transport-8.15.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from hurry.filesize>=0.9->news-please) (75.1.0)\n",
            "Requirement already satisfied: Pillow>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (6.0.2)\n",
            "Collecting feedparser>=6.0.0 (from newspaper4k>=0.9.3.1->news-please)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: nltk>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from newspaper4k>=0.9.3.1->news-please) (2.32.3)\n",
            "Collecting tldextract>=2.0.1 (from newspaper4k>=0.9.3.1->news-please)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from readability-lxml>=0.6.2->news-please) (5.2.0)\n",
            "Collecting cssselect (from readability-lxml>=0.6.2->news-please)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting Twisted>=18.9.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading twisted-24.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy>=1.1.0->news-please) (43.0.3)\n",
            "Collecting itemloaders>=1.0.1 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy>=1.1.0->news-please) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting zope.interface>=5.1.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting itemadapter>=0.1.0 (from Scrapy>=1.1.0->news-please)\n",
            "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Scrapy>=1.1.0->news-please) (24.2)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from Scrapy>=1.1.0->news-please) (0.7.1)\n",
            "Collecting botocore<1.36.0,>=1.35.63 (from boto3->news-please)\n",
            "  Downloading botocore-1.35.63-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->news-please)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->news-please)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis->news-please) (4.0.3)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.63->boto3->news-please) (2.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->Scrapy>=1.1.0->news-please) (1.17.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch>=2.4->news-please) (2024.8.30)\n",
            "Collecting sgmllib3k (from feedparser>=6.0.0->newspaper4k>=0.9.3.1->news-please)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (4.66.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->newspaper4k>=0.9.3.1->news-please) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->newspaper4k>=0.9.3.1->news-please) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->newspaper4k>=0.9.3.1->news-please) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->newspaper4k>=0.9.3.1->news-please) (3.10)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (24.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (0.4.1)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper4k>=0.9.3.1->news-please)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper4k>=0.9.3.1->news-please) (3.16.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->Scrapy>=1.1.0->news-please) (2.22)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (2.1.0)\n",
            "Downloading news_please-1.6.13-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.9/95.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Downloading elasticsearch-8.16.0-py3-none-any.whl (543 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.1/543.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faust_cchardet-2.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading newspaper4k-0.9.3.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
            "Downloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading warcio-1.7.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.63-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading redis-5.2.0-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.63-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading elastic_transport-8.15.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twisted-24.10.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: ago, hurry.filesize, langdetect, sgmllib3k\n",
            "  Building wheel for ago (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ago: filename=ago-0.0.95-py3-none-any.whl size=5158 sha256=a81f6e4045860655841f23c42e9c67168fc7e52fbac111b9cea63596b5b08ca0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/b3/4b/1a778dcc4ab767ee37335851d6df8922cf08cc8c67e3daf8f8\n",
            "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4091 sha256=80dfb91f90ba2a978b0c8df6a885cd93e651ae4617527eddd9ef99ff72ad6802\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/af/17/1c4cd045d88f20d450522470819d85349c3388c151af64590b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=e1c5423a3aa5f1bb78e79e3924e229d3d113e4c5ab6807614fe99645c78e7e9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=6df758e8c9fd4f5ff9115e1f36a4ea97c84a8aadc1ac6b183344fc5ff58ab2bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built ago hurry.filesize langdetect sgmllib3k\n",
            "Installing collected packages: sgmllib3k, PyDispatcher, plac, hjson, faust-cchardet, dotmap, ago, zope.interface, warcio, w3lib, redis, queuelib, PyMySQL, psycopg2-binary, protego, lxml-html-clean, langdetect, jmespath, itemadapter, incremental, hyperlink, hurry.filesize, feedparser, elastic-transport, cssselect, constantly, automat, Twisted, requests-file, readability-lxml, parsel, elasticsearch, bs4, botocore, tldextract, service-identity, s3transfer, itemloaders, Scrapy, newspaper4k, boto3, news-please\n",
            "Successfully installed PyDispatcher-2.0.7 PyMySQL-1.1.1 Scrapy-2.11.2 Twisted-24.10.0 ago-0.0.95 automat-24.8.1 boto3-1.35.63 botocore-1.35.63 bs4-0.0.2 constantly-23.10.4 cssselect-1.2.0 dotmap-1.3.30 elastic-transport-8.15.1 elasticsearch-8.16.0 faust-cchardet-2.1.19 feedparser-6.0.11 hjson-3.1.0 hurry.filesize-0.9 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.2 jmespath-1.0.1 langdetect-1.0.9 lxml-html-clean-0.4.1 news-please-1.6.13 newspaper4k-0.9.3.1 parsel-1.9.1 plac-1.4.3 protego-0.3.1 psycopg2-binary-2.9.10 queuelib-1.7.0 readability-lxml-0.8.1 redis-5.2.0 requests-file-2.1.0 s3transfer-0.10.3 service-identity-24.2.0 sgmllib3k-1.0.0 tldextract-5.1.3 w3lib-2.2.1 warcio-1.7.4 zope.interface-7.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper_scraper"
      ],
      "metadata": {
        "id": "Kd-sGSyqLgLh",
        "outputId": "6eae295b-8715-47b2-d5d9-25f60e0cec71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper_scraper\n",
            "  Downloading newspaper_scraper-0.2.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from newspaper_scraper) (2.2.2)\n",
            "Collecting selenium (from newspaper_scraper)\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from newspaper_scraper) (4.12.3)\n",
            "Collecting goose3==3.1.11 (from newspaper_scraper)\n",
            "  Downloading goose3-3.1.11-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from newspaper_scraper) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (11.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (5.3.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (1.2.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (0.42.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (3.9.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (2.8.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from goose3==3.1.11->newspaper_scraper) (1.0.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->newspaper_scraper) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->newspaper_scraper) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->newspaper_scraper) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->newspaper_scraper) (2024.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->newspaper_scraper) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium->newspaper_scraper)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->newspaper_scraper)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->newspaper_scraper) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium->newspaper_scraper) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->newspaper_scraper) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->goose3==3.1.11->newspaper_scraper) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->newspaper_scraper) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->newspaper_scraper)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->newspaper_scraper) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium->newspaper_scraper)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->newspaper_scraper) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->newspaper_scraper) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->newspaper_scraper)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->newspaper_scraper) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->goose3==3.1.11->newspaper_scraper) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->goose3==3.1.11->newspaper_scraper) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->goose3==3.1.11->newspaper_scraper) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->goose3==3.1.11->newspaper_scraper) (3.4.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->newspaper_scraper) (0.14.0)\n",
            "Downloading newspaper_scraper-0.2.1-py3-none-any.whl (31 kB)\n",
            "Downloading goose3-3.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, goose3, trio-websocket, selenium, newspaper_scraper\n",
            "Successfully installed goose3-3.1.11 newspaper_scraper-0.2.1 outcome-1.3.0.post0 selenium-4.26.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newsplease import NewsPlease\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import newspaper_scraper as nps\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv"
      ],
      "metadata": {
        "id": "Uz32WsCztVHp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to deal with URL Scraping'"
      ],
      "metadata": {
        "id": "4iKoKhYklufs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the domain (i.e., the newspaper/source) from the URL\n",
        "def extract_newspaper(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "    return domain.replace('www.', '')  # Clean up 'www.' if present"
      ],
      "metadata": {
        "id": "Ye1NHNkan2zS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape an article from a URL and return relevant information\n",
        "def scrape_article(url):\n",
        "    try:\n",
        "        article = NewsPlease.from_url(url)\n",
        "        return {\n",
        "            'title': article.title,\n",
        "            'authors': article.authors,\n",
        "            'date_publish': article.date_publish,\n",
        "            'maintext': article.maintext,\n",
        "            'url': url,\n",
        "            'newspaper': extract_newspaper(url)  # Add newspaper information\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Return None if there's any error (e.g., 404, timeout)\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Q16b25Kx11c1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to scrape multicle articles and return a dataframe with relevant information\n",
        "def scrape_articles(url_list):\n",
        "    # Initialize an empty list to store scraped article data\n",
        "    articles_data = []\n",
        "\n",
        "    # Loop through each URL and scrape the article\n",
        "    for url in url_list:\n",
        "        article_data = scrape_article(url)\n",
        "        if article_data:\n",
        "            articles_data.append(article_data)\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame\n",
        "    df = pd.DataFrame(articles_data)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xUC4EUKG2MXd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Usage of Functions"
      ],
      "metadata": {
        "id": "EhIWQQZFl0hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Usage\n",
        "urls = [\n",
        "    \"https://www.abendblatt.de/hamburg/hamburg-mitte/article407529183/elbtower-kauft-tuerkische-firma-hamburgs-kuenftiges-wahrzeichen.html\",\n",
        "    \"https://www.sueddeutsche.de/politik/israel-hisbollah-libanon-bank-gold-geld-krankenhaus-lux.TxxwzAtVsoRuvgqkHfMiuy\",\n",
        "    \"https://www.morgenpost.de/politik/article407533911/gruene-verbieten-hunde-wie-csu-zu-diesem-schwachsinn-kommt.html\"\n",
        "]\n",
        "\n",
        "# Scrape articles and create a DataFrame\n",
        "articles_df = scrape_articles(urls)\n",
        "\n",
        "# Display the dataframe\n",
        "print(articles_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NENtyFtq2RuH",
        "outputId": "7a7fc238-e220-4c0a-e3d0-91e4a7cb5398"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title            authors  \\\n",
            "0  Elbtower Hamburg: Drei Kaufinteressenten sind ...   [Ulrich Gaßdorf]   \n",
            "1   Israels Armee sucht den Goldschatz der Hisbollah    [Bernd Dörries]   \n",
            "2  Grüne verbieten Hunde? Wie CSU zu diesem „Schw...  [Daniel Weidmann]   \n",
            "\n",
            "         date_publish                                           maintext  \\\n",
            "0 2024-10-28 13:25:00  Hamburg. Zu den möglichen Investoren soll eine...   \n",
            "1 2024-10-22 14:56:01  Der Direktor des Krankenhauses, ein Parlaments...   \n",
            "2 2024-10-24 11:18:36  Berlin. Der CSU-Generalsekretär behauptet, die...   \n",
            "\n",
            "                                                 url        newspaper  \n",
            "0  https://www.abendblatt.de/hamburg/hamburg-mitt...    abendblatt.de  \n",
            "1  https://www.sueddeutsche.de/politik/israel-his...  sueddeutsche.de  \n",
            "2  https://www.morgenpost.de/politik/article40753...    morgenpost.de  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8qpOyQt8PIM",
        "outputId": "43a362b8-3ad6-448c-93be-477b3ec5253d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   title         3 non-null      object        \n",
            " 1   authors       3 non-null      object        \n",
            " 2   date_publish  3 non-null      datetime64[ns]\n",
            " 3   maintext      3 non-null      object        \n",
            " 4   url           3 non-null      object        \n",
            " 5   newspaper     3 non-null      object        \n",
            "dtypes: datetime64[ns](1), object(5)\n",
            "memory usage: 272.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "le6Mv0K8bkXi",
        "outputId": "dafe901e-0324-4cc8-8ba3-93fe9e01d6ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title            authors  \\\n",
              "0  Elbtower Hamburg: Drei Kaufinteressenten sind ...   [Ulrich Gaßdorf]   \n",
              "1   Israels Armee sucht den Goldschatz der Hisbollah    [Bernd Dörries]   \n",
              "2  Grüne verbieten Hunde? Wie CSU zu diesem „Schw...  [Daniel Weidmann]   \n",
              "\n",
              "         date_publish                                           maintext  \\\n",
              "0 2024-10-28 13:25:00  Hamburg. Zu den möglichen Investoren soll eine...   \n",
              "1 2024-10-22 14:56:01  Der Direktor des Krankenhauses, ein Parlaments...   \n",
              "2 2024-10-24 11:18:36  Berlin. Der CSU-Generalsekretär behauptet, die...   \n",
              "\n",
              "                                                 url        newspaper  \n",
              "0  https://www.abendblatt.de/hamburg/hamburg-mitt...    abendblatt.de  \n",
              "1  https://www.sueddeutsche.de/politik/israel-his...  sueddeutsche.de  \n",
              "2  https://www.morgenpost.de/politik/article40753...    morgenpost.de  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66a901fb-8463-4d36-a470-cb2195503737\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>date_publish</th>\n",
              "      <th>maintext</th>\n",
              "      <th>url</th>\n",
              "      <th>newspaper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Elbtower Hamburg: Drei Kaufinteressenten sind ...</td>\n",
              "      <td>[Ulrich Gaßdorf]</td>\n",
              "      <td>2024-10-28 13:25:00</td>\n",
              "      <td>Hamburg. Zu den möglichen Investoren soll eine...</td>\n",
              "      <td>https://www.abendblatt.de/hamburg/hamburg-mitt...</td>\n",
              "      <td>abendblatt.de</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Israels Armee sucht den Goldschatz der Hisbollah</td>\n",
              "      <td>[Bernd Dörries]</td>\n",
              "      <td>2024-10-22 14:56:01</td>\n",
              "      <td>Der Direktor des Krankenhauses, ein Parlaments...</td>\n",
              "      <td>https://www.sueddeutsche.de/politik/israel-his...</td>\n",
              "      <td>sueddeutsche.de</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Grüne verbieten Hunde? Wie CSU zu diesem „Schw...</td>\n",
              "      <td>[Daniel Weidmann]</td>\n",
              "      <td>2024-10-24 11:18:36</td>\n",
              "      <td>Berlin. Der CSU-Generalsekretär behauptet, die...</td>\n",
              "      <td>https://www.morgenpost.de/politik/article40753...</td>\n",
              "      <td>morgenpost.de</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a901fb-8463-4d36-a470-cb2195503737')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66a901fb-8463-4d36-a470-cb2195503737 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66a901fb-8463-4d36-a470-cb2195503737');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-47148bfa-d9c8-40f0-837d-50e84f1515ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47148bfa-d9c8-40f0-837d-50e84f1515ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-47148bfa-d9c8-40f0-837d-50e84f1515ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "articles_df",
              "summary": "{\n  \"name\": \"articles_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Elbtower Hamburg: Drei Kaufinteressenten sind noch im Rennen\",\n          \"Israels Armee sucht den Goldschatz der Hisbollah\",\n          \"Gr\\u00fcne verbieten Hunde? Wie CSU zu diesem \\u201eSchwachsinn\\u201c kommt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date_publish\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-10-22 14:56:01\",\n        \"max\": \"2024-10-28 13:25:00\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-10-28 13:25:00\",\n          \"2024-10-22 14:56:01\",\n          \"2024-10-24 11:18:36\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"maintext\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Hamburg. Zu den m\\u00f6glichen Investoren soll eine bekannte t\\u00fcrkische Firma geh\\u00f6ren. Doch auch zwei Konsortien aus Hamburg hoffen auf den Zuschlag.\",\n          \"Der Direktor des Krankenhauses, ein Parlamentsabgeordneter der Hisbollah-nahen Amal-Partei, bestritt die Vorw\\u00fcrfe, lie\\u00df das Krankenhaus am Dienstag aber r\\u00e4umen und Journalisten ins Geb\\u00e4ude hinein, gleichzeitig twitterte ein israelischer Armeesprecher Aufforderungen, man solle sich nicht blenden lassen und in den Keller gehen, wo das Geld liege. Gefunden wurde offenbar nichts.\\nDie Hausbank der Terrororganisation soll Gelder aus deren kriminellen Aktivit\\u00e4ten gewaschen haben\\nDie israelische Armee hatte in den vergangenen Tagen immer wieder das Finanzsystem der Hisbollah ins Visier genommen, vor allem die Al-Kard-Al-Hassan-Bank, mindestens 20 ihrer Filialen in Beirut und anderen Landesteilen wurden angegriffen und dabei teilweise ganze Wohngeb\\u00e4ude zerst\\u00f6rt, in deren Erdgeschoss sich die Zweigstellen befanden. Laut israelischer Armee werden durch Al-Kard Al-Hassan der Terror finanziert, Hisbollah-K\\u00e4mpfer bezahlt und Waffenk\\u00e4ufe erm\\u00f6glicht. Bereits bei der T\\u00f6tung von Hassan Nasrallah Ende Oktober sollen gro\\u00dfe Mengen Bargeld durch die Bomben auf den Bunker des Hisbollah-F\\u00fchrers vernichtet worden sein. Al-Kard Al-Hassan wurde in der Vergangenheit von westlichen Geheimdiensten vorgeworfen, Gelder aus dem Drogenhandel und kriminellen Aktivit\\u00e4ten der Hisbollah zu waschen oder ins Land zu transferieren.\\nAl-Kard Al-Hassan bedeutet so etwas wie \\u201eDas gute Darlehen\\u201c, es ist eine Finanzinstitution nach islamischen Regeln, die Zinsen und Wucher verbieten. Die Bank wurde 1983 gegr\\u00fcndet und soll derzeit etwa 300 000 Privatkunden haben, vor allem schiitische Libanesen. Hisbollah-F\\u00fchrer Nasrallah pries die Bank 2021 in einer Rede als gro\\u00dfen Gewinn f\\u00fcr die schiitische Gemeinschaft, seit ihrem Bestehen habe sie 3,7 Milliarden Dollar Kredite an 1,8 Millionen Kunden vergeben.\\nDer Finanzier Iran ist klamm, das hat Folgen f\\u00fcr die Hisbollah\\nBei einem Gro\\u00dfteil d\\u00fcrfte es sich um Kleinkredite gehandelt haben, viele Schiiten bringen Gold oder Familienschmuck zu Al-Kard Al-Hassan und erhalten im Gegenzug ein Darlehen f\\u00fcr Schulgeb\\u00fchren oder Beerdigungen. Die Hisbollah soll vielen Kunden bereits per Whatsapp versichert haben, dass ihre Einlagen noch da sind. Die Bank hatte w\\u00e4hrend der libanesischen Wirtschaftskrise, die 2019 begann und die Ersparnisse vieler Menschen vernichtete, an Reputation gewonnen. W\\u00e4hrend Kunden des offiziellen Sektors nicht mehr an ihre Dollar kamen und die lokale W\\u00e4hrung 97 Prozent ihres Wertes verlor, \\u00e4nderte sich f\\u00fcr die Kunden der Al-Kard-Al-Hassan-Bank nichts, sie kamen weiter an ihr Geld.\\nDie Finanzsituation der Hisbollah insgesamt scheint sich aber in den vergangenen Jahren verschlechtert zu haben, der gro\\u00dfe Finanzier Iran ist in einer schweren Wirtschaftskrise. Vor allem im S\\u00fcden Libanons w\\u00e4chst die Wut auf die Hisbollah, weil sie offenbar nicht das Geld hat, die von Israel zerst\\u00f6rten H\\u00e4user und Einrichtungen wieder aufzubauen. Oder \\u00fcberhaupt nennenswerte humanit\\u00e4re Hilfe f\\u00fcr die mehr als eine Million Menschen zu leisten, die ihre H\\u00e4user verlassen mussten. Das war nach dem Krieg 2006 noch anders, damals soll es f\\u00fcr jedes zerst\\u00f6rte Haus etwa 120 000 Dollar gegeben haben.\\nViele libanesische Hisbollah-Analysten denken, dass die Angriffe auf das Finanzsystem nicht so sehr dazu dienen, die milit\\u00e4rische Kraft der Hisbollah zu schw\\u00e4chen, sondern eher psychologischer Natur sind, um die Anh\\u00e4ngerschaft zu verunsichern.\\nDen angeblichen Goldschatz unter dem Krankenhaus werde man nicht attackieren, hatte der israelische Armeesprecher noch am Montag gesagt, der Krieg richte sich nicht gegen das libanesische Volk. Wenig sp\\u00e4ter traf eine Bombe die Nachbarschaft des Rafik-Hariri-Krankenhauses, 13 Menschen wurden get\\u00f6tet.\",\n          \"Berlin. Der CSU-Generalsekret\\u00e4r behauptet, die Gr\\u00fcne Jugend will Hunde verbieten. Woher das Narrativ kommt \\u2013 und warum Lang auf Trump verweist.\\nAus ihrer Ablehnung gr\\u00fcner Politik, sogar der ganzen Partei, macht die CSU keinen Hehl: Gebetsm\\u00fchlenartig wiederholen f\\u00fchrende K\\u00f6pfe der bayrischen Schwesterpartei ihr Mantra, das sich zwischen \\u201eNein zu Schwarz-Gr\\u00fcn\\u201c und \\u201eGr\\u00fcne sind gegen Bayern\\u201c einpendelt. So zumindest formulierte es CSU-Chef und Ministerpr\\u00e4sident Markus S\\u00f6der bei seiner Parteitagsrede.\\nWas zun\\u00e4chst als eindeutige Koalitionsabsage begann, scheint jetzt zur Wahlkampfstrategie der CSU zu werden, die in \\u00fcberspitzter Form die sozialen Medien erreicht. Dort postete CSU-General Martin Huber ein Video, indem er sich als Besch\\u00fctzer von Haustierbesitzern inszeniert. Seine These: Die Gr\\u00fcne Jugend will Hundebesitzern ihre Haustiere wegnehmen.\\nCSU postet Video: Woher kommt der Vorwurf?\\n\\u201eSie sind f\\u00fcr viele Wegbegleiter, Kummerkasten, Familienmitglied und Spielgef\\u00e4hrte\\u201c, spricht er in die Kamera, w\\u00e4hrend ein Hund vor ihm auf und ab h\\u00fcpft. Hinterlegt wird das Video, das der Generalsekret\\u00e4r unter anderem via Instagram verbreitete, mit Klaviermusik. \\u201eUnd die Gr\\u00fcne Jugend m\\u00f6chte Euch das verbieten und einen treuen Freund wegnehmen. Was f\\u00fcr ein herzloser Irrsinn\\u201c, res\\u00fcmiert Huber.\\nHubers These st\\u00fctzt sich auf einen Beitrag der \\u201eBild\\u201c, den er zu Beginn seines Videos als Screenshot einblendet und den die CSU als Sharepic verbreitet. \\u201eWelpen-Feind ist neuer Junior-Chef der Gr\\u00fcnen\\u201c titelte das Medium, nachdem Jakob Blasel und Jette Nietzard zum neuen F\\u00fchrungsduo der Gr\\u00fcnen Jugend bestimmt wurden.\\nAuch interessant\\nRicarda Lang vergleicht Hubers Aussage mit der von Trump\\nDie \\u201eBild\\u201c leitete diese Behauptung aus einem Interview ab, das Blasel 2019 dem ARD-Jugendformat Funk gegeben hatte. \\u201eSo liebensw\\u00fcrdig unsere Haustiere auch sind, wir brauchen sie eigentlich nicht, und das ist ein ziemlicher Umwelt- und CO\\u2082-Luxus, den wir uns da leisten\\u201c, zitieren verschiedene Medien aus dem Video, das nicht mehr online ist.\\nDer Fridays For Future Mitbegr\\u00fcnder pl\\u00e4dierte demnach daf\\u00fcr, es zu verbieten, die Tiere unn\\u00f6tig zu z\\u00fcchten. Unter anderem der Bayrische Rundfunk zitiert Blasel mit der Empfehlung, \\u201evor allem Tiere aus dem Tierheim zu nehmen, wenn wir denn gern ein Haustier h\\u00e4tten\\u201c. Von einer Forderung nach einem Haustierverbot: keine Spur.\\nIn den sozialen Medien traf Hubers Video bei f\\u00fchrenden K\\u00f6pfen der Gr\\u00fcnen auf wenig Gegenliebe und H\\u00e4me. In einem Video verweist die scheidende Gr\\u00fcnen-Vorsitzende Ricarda Lang auf die Aussage Donald Trumps, der behauptet hatte, dass Haitianer in den USA Katzen und Hunde essen w\\u00fcrden. \\u201eUnd da hat sich die CDU gedacht: Was Donald kann, das k\\u00f6nnen wir schon l\\u00e4nger\\u201c, so Lang via X. \\u201eDas ist nat\\u00fcrlich kompletter Schwachsinn.\\u201c\\nGr\\u00fcnen-Europaabgeordneter Erik Marquardt kommentierte neben Hubers Beitrag: \\u201eDas C in CSU steht auf jeden Fall f\\u00fcr crazy.\\u201c\\nAuch interessant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"https://www.abendblatt.de/hamburg/hamburg-mitte/article407529183/elbtower-kauft-tuerkische-firma-hamburgs-kuenftiges-wahrzeichen.html\",\n          \"https://www.sueddeutsche.de/politik/israel-hisbollah-libanon-bank-gold-geld-krankenhaus-lux.TxxwzAtVsoRuvgqkHfMiuy\",\n          \"https://www.morgenpost.de/politik/article407533911/gruene-verbieten-hunde-wie-csu-zu-diesem-schwachsinn-kommt.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"newspaper\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"abendblatt.de\",\n          \"sueddeutsche.de\",\n          \"morgenpost.de\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(articles_df.loc[0, 'maintext'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX258_Voe1fR",
        "outputId": "1b0f6703-9413-4692-9c86-bb1deb64247c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamburg. Zu den möglichen Investoren soll eine bekannte türkische Firma gehören. Doch auch zwei Konsortien aus Hamburg hoffen auf den Zuschlag.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with nps.DeBild(db_files='articles_debild.db') as news: #this does not compile, we should find a way to write files while using colab\n",
        "  #news.index_articles_by_date_range('2023-09-01', '2023-09-30') #fron 1st to 30th september 2023\n",
        "  #news.scrape_public_articles()\n",
        "  #it's also possible to apply spacy's nlp function directly with the scraping library\n",
        "  #news.nlp()"
      ],
      "metadata": {
        "id": "RIjyMLQsimSn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping of URLs into a CSV File"
      ],
      "metadata": {
        "id": "MFzUW-5wl5v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE 1\n",
        "# List of news websites to scrape\n",
        "urls_to_scrape = [\n",
        "    \"https://www.spiegel.de\",   # Der Spiegel\n",
        "    \"https://www.zeit.de\",      # Die Zeit\n",
        "    \"https://www.sueddeutsche.de\",  # Süddeutsche Zeitung\n",
        "    \"https://www.faz.net\", #Frankfurter Allgemeine\n",
        "    \"https://www.tagesspiegel.de\", #Tagesspiegel\n",
        "    \"https://www.n-tv.de\" #n.tv\n",
        "]\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
        "}\n",
        "\n",
        "article_urls = set()  # Use a set to avoid duplicates\n",
        "\n",
        "# Patterns that likely indicate article URLs\n",
        "article_patterns = [\"/artikel/\", \"/news/\", \"/politik/\", \"/wirtschaft/\", \"/sport/\", \"/202\", \"/2023\", \"/2024\"]\n",
        "\n",
        "# Exclude URLs with these keywords\n",
        "exclude_keywords = [\"contact\", \"about\", \"impressum\", \"privacy\", \"datenschutz\", \"jobs\", \"login\"]\n",
        "\n",
        "for base_url in urls_to_scrape:\n",
        "    try:\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        parsed_base_url = urlparse(base_url).netloc\n",
        "\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(base_url, href)  # Convert to absolute URL\n",
        "\n",
        "            # Ensure the URL belongs to the base domain and matches article patterns\n",
        "            parsed_url = urlparse(full_url)\n",
        "            if (parsed_url.netloc == parsed_base_url and\n",
        "                any(pattern in full_url for pattern in article_patterns) and\n",
        "                not any(keyword in full_url for keyword in exclude_keywords)):\n",
        "                article_urls.add(full_url)\n",
        "\n",
        "        # Stop when we have 1000 unique URLs\n",
        "        if len(article_urls) >= 1000:\n",
        "            break\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {base_url}: {e}\")\n",
        "\n",
        "# Save URLs to a CSV file\n",
        "with open(\"filtered_news_article_urls.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    for article_url in list(article_urls)[:1000]:  # Limit to 1000 URLs\n",
        "        writer.writerow([article_url])\n",
        "\n",
        "print(f\"Collected {len(article_urls)} article URLs. saved to filtered_news_article_urls.csv\")"
      ],
      "metadata": {
        "id": "JkCNt5Q8wTSg",
        "outputId": "762fa241-c66b-4c8a-ca68-f6c73809d14c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 544 article URLs. saved to filtered_news_article_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE 2\n",
        "# List of news websites to scrape\n",
        "urls_to_scrape = [\n",
        "    \"https://www.spiegel.de\",   # Der Spiegel\n",
        "    \"https://www.zeit.de\",      # Die Zeit\n",
        "    \"https://www.sueddeutsche.de\"  # Süddeutsche Zeitung\n",
        "]\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
        "}\n",
        "\n",
        "article_urls = set()  # Use a set to avoid duplicates\n",
        "\n",
        "# Patterns likely indicating article URLs\n",
        "article_patterns = [\"/artikel/\", \"/news/\", \"/politik/\", \"/wirtschaft/\", \"/sport/\", \"/202\", \"/2023\", \"/2024\"]\n",
        "\n",
        "# Exclude URLs with these keywords\n",
        "exclude_keywords = [\"contact\", \"about\", \"impressum\", \"privacy\", \"datenschutz\", \"jobs\", \"login\", \"feedback\"]\n",
        "\n",
        "def scrape_page(base_url):\n",
        "    \"\"\"Scrape a single page for article links.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        parsed_base_url = urlparse(base_url).netloc\n",
        "        links_to_follow = set()\n",
        "\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(base_url, href)  # Convert to absolute URL\n",
        "\n",
        "            # Check if the URL belongs to the base domain and matches article patterns\n",
        "            parsed_url = urlparse(full_url)\n",
        "            if parsed_url.netloc == parsed_base_url:\n",
        "                if any(pattern in full_url for pattern in article_patterns) and not any(keyword in full_url for keyword in exclude_keywords):\n",
        "                    article_urls.add(full_url)\n",
        "                elif \"/seite/\" in full_url or \"page=\" in full_url:  # Collect pagination links\n",
        "                    links_to_follow.add(full_url)\n",
        "\n",
        "        return links_to_follow\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {base_url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "# Scrape the homepage and collect pagination links\n",
        "for base_url in urls_to_scrape:\n",
        "    to_visit = scrape_page(base_url)  # Initial scrape\n",
        "    visited = set()\n",
        "\n",
        "    # Follow pagination and section links\n",
        "    while to_visit and len(article_urls) < 2000:\n",
        "        next_url = to_visit.pop()\n",
        "        if next_url not in visited:\n",
        "            visited.add(next_url)\n",
        "            to_visit.update(scrape_page(next_url))\n",
        "\n",
        "# Save URLs to a CSV file\n",
        "with open(\"expanded_news_article_urls.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    for article_url in list(article_urls)[:2000]:  # Limit to 2000 URLs\n",
        "        writer.writerow([article_url])\n",
        "\n",
        "print(f\"Collected {len(article_urls)} article URLs. Saved to expanded_news_article_urls.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YVIhOwgvUza",
        "outputId": "1981fed9-d7ce-44d8-8257-6859c2a48dc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 272 article URLs. Saved to expanded_news_article_urls.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE 3\n",
        "# List of German news websites to scrape\n",
        "urls_to_scrape = [\n",
        "    \"https://www.spiegel.de\",   # Der Spiegel\n",
        "    \"https://www.zeit.de\",      # Die Zeit\n",
        "    \"https://www.sueddeutsche.de\",  # Süddeutsche Zeitung\n",
        "    \"https://www.faz.net\",       # Frankfurter Allgemeine\n",
        "    \"https://www.tagesspiegel.de\", # Tagesspiegel\n",
        "    \"https://www.n-tv.de\",       # n.tv\n",
        "    \"https://www.welt.de\",       # Die Welt\n",
        "    \"https://www.focus.de\",      # Focus Online\n",
        "    \"https://www.handelsblatt.com\", # Handelsblatt\n",
        "    \"https://www.taz.de\"         # taz\n",
        "]\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
        "}\n",
        "\n",
        "article_urls = set()  # Use a set to avoid duplicates\n",
        "\n",
        "# Patterns likely indicating article URLs\n",
        "article_patterns = [\"/artikel/\", \"/news/\", \"/politik/\", \"/wirtschaft/\", \"/sport/\", \"/202\", \"/2023\", \"/2024\"]\n",
        "\n",
        "# Exclude URLs with these keywords\n",
        "exclude_keywords = [\"contact\", \"about\", \"impressum\", \"privacy\", \"datenschutz\", \"jobs\", \"login\", \"feedback\"]\n",
        "\n",
        "def scrape_page(base_url):\n",
        "    \"\"\"Scrape a single page for article links and pagination links.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(base_url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        parsed_base_url = urlparse(base_url).netloc\n",
        "        links_to_follow = set()\n",
        "\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(base_url, href)  # Convert to absolute URL\n",
        "\n",
        "            # Check if the URL belongs to the base domain and matches article patterns\n",
        "            parsed_url = urlparse(full_url)\n",
        "            if parsed_url.netloc == parsed_base_url:\n",
        "                if any(pattern in full_url for pattern in article_patterns) and not any(keyword in full_url for keyword in exclude_keywords):\n",
        "                    article_urls.add(full_url)\n",
        "                elif \"/seite/\" in full_url or \"page=\" in full_url or \"p=\" in full_url:  # Collect pagination links\n",
        "                    links_to_follow.add(full_url)\n",
        "\n",
        "        return links_to_follow\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {base_url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "# Scrape each website and collect URLs\n",
        "for base_url in urls_to_scrape:\n",
        "    print(f\"Scraping {base_url}...\")\n",
        "    to_visit = scrape_page(base_url)  # Initial scrape\n",
        "    visited = set()\n",
        "\n",
        "    # Follow pagination and section links\n",
        "    while to_visit and len(article_urls) < 10000:  # Stop after collecting 10,000 URLs\n",
        "        next_url = to_visit.pop()\n",
        "        if next_url not in visited:\n",
        "            visited.add(next_url)\n",
        "            to_visit.update(scrape_page(next_url))\n",
        "\n",
        "# Save URLs to a CSV file with proper column naming\n",
        "output_file = \"german_news_article_urls.csv\"\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"URLs\"])  # Column name\n",
        "    for article_url in sorted(article_urls):  # Sort URLs for better organization\n",
        "        writer.writerow([article_url])\n",
        "\n",
        "print(f\"Scraping completed! Collected {len(article_urls)} article URLs. Saved to {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeaYcOlYVMrj",
        "outputId": "4b540889-f6c7-4446-bf32-b8386a36940c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://www.spiegel.de...\n",
            "Scraping https://www.zeit.de...\n",
            "Scraping https://www.sueddeutsche.de...\n",
            "Scraping https://www.faz.net...\n",
            "Scraping https://www.tagesspiegel.de...\n",
            "Scraping https://www.n-tv.de...\n",
            "Scraping https://www.welt.de...\n",
            "Scraping https://www.focus.de...\n",
            "Scraping https://www.handelsblatt.com...\n",
            "Scraping https://www.taz.de...\n",
            "Scraping completed! Collected 726 article URLs. Saved to german_news_article_urls.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls_file = pd.read_csv(\"filtered_news_article_urls.csv\")\n",
        "urls_file2 = pd.read_csv(\"expanded_news_article_urls.csv\")\n",
        "urls_file3 = pd.read_csv(\"german_news_article_urls.csv\")"
      ],
      "metadata": {
        "id": "tgZRRif7xbMn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls_file3.head(10)"
      ],
      "metadata": {
        "id": "24nUs8qSxk9_",
        "outputId": "54a4c297-182e-40c2-b075-17d64db9b283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                URLs\n",
              "0               https://www.faz.net/aktuell/politik/\n",
              "1  https://www.faz.net/aktuell/politik/antikriegs...\n",
              "2       https://www.faz.net/aktuell/politik/ausland/\n",
              "3  https://www.faz.net/aktuell/politik/briefe-an-...\n",
              "4    https://www.faz.net/aktuell/politik/geschichte/\n",
              "5        https://www.faz.net/aktuell/politik/inland/\n",
              "6  https://www.faz.net/aktuell/politik/inland/amp...\n",
              "7  https://www.faz.net/aktuell/politik/inland/fre...\n",
              "8  https://www.faz.net/aktuell/politik/inland/gru...\n",
              "9  https://www.faz.net/aktuell/politik/inland/hab..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7666500-07c2-40ca-bb1f-be1972743420\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URLs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/antikriegs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/ausland/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/briefe-an-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/geschichte/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/inland/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/inland/amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/inland/fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/inland/gru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>https://www.faz.net/aktuell/politik/inland/hab...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7666500-07c2-40ca-bb1f-be1972743420')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7666500-07c2-40ca-bb1f-be1972743420 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7666500-07c2-40ca-bb1f-be1972743420');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1430a3c3-99d8-4ed3-b37a-d0627106671c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1430a3c3-99d8-4ed3-b37a-d0627106671c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1430a3c3-99d8-4ed3-b37a-d0627106671c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "urls_file3",
              "summary": "{\n  \"name\": \"urls_file3\",\n  \"rows\": 726,\n  \"fields\": [\n    {\n      \"column\": \"URLs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 726,\n        \"samples\": [\n          \"https://www.spiegel.de/sport/tennis/atp-finals-doppel-krawietz-und-puetz-gewinnt-titel-bei-saisonfinale-a-1b82020b-7de4-4c40-8591-05ea7a1a7f27\",\n          \"https://www.welt.de/politik/deutschland/article254558056/Bundeskanzler-Scholz-verteidigt-Telefonat-mit-Putin.html\",\n          \"https://www.welt.de/wirtschaft/plus254491184/Versicherung-Rekordhoch-bei-der-Kfz-Police-So-senken-Sie-Ihre-Beitraege-jetzt-drastisch.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls_file3.info()"
      ],
      "metadata": {
        "id": "6_rUzSN7xmR3",
        "outputId": "0c0f55f9-1a54-4ede-c8f2-a512b31f93e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 726 entries, 0 to 725\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   URLs    726 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 5.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Had to go to work\n",
        "## Finish anpassing of code to read from file and not from URL\n",
        "### Look into the https://github.com/fhamborg/news-please\n",
        "### Can we run the craweler (via the CLI) and give it our URLS to store the data defautly in Json files? ALTERNATIVE TO SCRAPING URLS through BeautifulSoup"
      ],
      "metadata": {
        "id": "esBBnVKD2mVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from newsplease import NewsPlease\n",
        "import urllib.parse\n",
        "\n",
        "# Function to read URLs from a CSV file\n",
        "def read_urls_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file and extracts URLs from the 'url' column.\n",
        "    :param file_path: Path to the CSV file\n",
        "    :return: List of URLs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'url' not in df.columns:\n",
        "            raise ValueError(\"The CSV file must have a column named 'url'\")\n",
        "        return df['url'].dropna().tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to scrape multiple articles and return a DataFrame with relevant information\n",
        "def scrape_articles_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Scrapes articles from URLs listed in a CSV file.\n",
        "    :param file_path: Path to the CSV file containing URLs\n",
        "    :return: DataFrame with scraped article information\n",
        "    \"\"\"\n",
        "    # Read URLs from the CSV\n",
        "    url_list = read_urls_from_csv(file_path)\n",
        "\n",
        "    if not url_list:\n",
        "        print(\"No URLs found to scrape.\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    # Initialize an empty list to store scraped article data\n",
        "    articles_data = []\n",
        "\n",
        "    # Loop through each URL and scrape the article\n",
        "    for url in url_list:\n",
        "        article_data = scrape_article(url)\n",
        "        if article_data:\n",
        "            articles_data.append(article_data)\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame\n",
        "    return pd.DataFrame(articles_data)\n",
        "\n",
        "# Function to scrape an article from a URL and return relevant information\n",
        "def scrape_article(url):\n",
        "    \"\"\"\n",
        "    Scrapes a single article from a URL and extracts relevant details.\n",
        "    :param url: URL of the article to scrape\n",
        "    :return: Dictionary with article details or None if scraping fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        article = NewsPlease.from_url(url)\n",
        "        return {\n",
        "            'title': article.title,\n",
        "            'authors': article.authors,\n",
        "            'date_publish': article.date_publish,\n",
        "            'maintext': article.maintext,\n",
        "            'url': url,\n",
        "            'newspaper': extract_newspaper(url)  # Add newspaper information\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Return None if there's any error (e.g., 404, timeout)\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract the domain (i.e., the newspaper/source) from the URL\n",
        "def extract_newspaper(url):\n",
        "    \"\"\"\n",
        "    Extracts the domain (newspaper/source) from the URL.\n",
        "    :param url: URL of the article\n",
        "    :return: Domain name of the source\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "    return domain.replace('www.', '')  # Clean up 'www.' if present\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"german_news_article_urls.csv\"\n",
        "scraped_data = scrape_articles_from_csv(file_path)\n",
        "print(scraped_data)"
      ],
      "metadata": {
        "id": "y4_t4JAP2oNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40a7bf8-5604-4ceb-c29e-48cc75090c55"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: The CSV file must have a column named 'url'\n",
            "No URLs found to scrape.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read URLs from a CSV file\n",
        "def read_urls_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file and extracts URLs from the 'URLs' column.\n",
        "    :param file_path: Path to the CSV file\n",
        "    :return: List of URLs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'URLs' not in df.columns:\n",
        "            raise ValueError(\"The CSV file must have a column named 'URLs'\")\n",
        "        return df['URLs'].dropna().tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to scrape multiple articles and return a DataFrame with relevant information\n",
        "def scrape_articles_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Scrapes articles from URLs listed in a CSV file.\n",
        "    :param file_path: Path to the CSV file containing URLs\n",
        "    :return: DataFrame with scraped article information\n",
        "    \"\"\"\n",
        "    # Read URLs from the CSV\n",
        "    url_list = read_urls_from_csv(file_path)\n",
        "\n",
        "    if not url_list:\n",
        "        print(\"No URLs found to scrape.\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    # Initialize an empty list to store scraped article data\n",
        "    articles_data = []\n",
        "\n",
        "    # Loop through each URL and scrape the article\n",
        "    for url in url_list:\n",
        "        article_data = scrape_article(url)\n",
        "        if article_data:\n",
        "            articles_data.append(article_data)\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame\n",
        "    return pd.DataFrame(articles_data)\n",
        "\n",
        "# Function to scrape an article from a URL and return relevant information\n",
        "def scrape_article(url):\n",
        "    \"\"\"\n",
        "    Scrapes a single article from a URL and extracts relevant details.\n",
        "    :param url: URL of the article to scrape\n",
        "    :return: Dictionary with article details or None if scraping fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        article = NewsPlease.from_url(url)\n",
        "        return {\n",
        "            'title': article.title,\n",
        "            'authors': article.authors,\n",
        "            'date_publish': article.date_publish,\n",
        "            'maintext': article.maintext,\n",
        "            'url': url,\n",
        "            'newspaper': extract_newspaper(url)  # Add newspaper information\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Return None if there's any error (e.g., 404, timeout)\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract the domain (i.e., the newspaper/source) from the URL\n",
        "def extract_newspaper(url):\n",
        "    \"\"\"\n",
        "    Extracts the domain (newspaper/source) from the URL.\n",
        "    :param url: URL of the article\n",
        "    :return: Domain name of the source\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "    return domain.replace('www.', '')  # Clean up 'www.' if present\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"german_news_article_urls.csv\"\n",
        "scraped_data = scrape_articles_from_csv(file_path)\n",
        "print(scraped_data)\n"
      ],
      "metadata": {
        "id": "FjS1DMnSpQdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081f30c1-bd3b-4273-ac19-affd03f5a852"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 title  \\\n",
            "0    Politik: Aktuelle Nachrichten aus dem In- und ...   \n",
            "1    Antikriegs-Marsch: Exil-Russen demonstrieren i...   \n",
            "2                        Ausland: Aktuelle Nachrichten   \n",
            "3                            Briefe an die Herausgeber   \n",
            "4        Geschichte: Historische Themen aus aller Welt   \n",
            "..                                                 ...   \n",
            "721  Zigerknödel mit Zwiebelsoße: Ein besonders käs...   \n",
            "722  Günstiges Essen in New York: Paradies für 17 D...   \n",
            "723     Wildschweinschulter: Pašticada vom Wildschwein   \n",
            "724               Love Scamming: Wer kennt diese Frau?   \n",
            "725               Love Scamming: Wer kennt diese Frau?   \n",
            "\n",
            "                            authors        date_publish  \\\n",
            "0                [www.facebook.com]                 NaT   \n",
            "1                [www.facebook.com] 2024-11-17 15:49:56   \n",
            "2                [www.facebook.com]                 NaT   \n",
            "3                [www.facebook.com]                 NaT   \n",
            "4                [www.facebook.com]                 NaT   \n",
            "..                              ...                 ...   \n",
            "721     [Eva Biringer, www.zeit.de] 2024-11-17 14:45:23   \n",
            "722  [Claire Beermann, www.zeit.de] 2024-11-17 17:02:37   \n",
            "723  [Silvio Knezevic, www.zeit.de] 2024-11-17 11:48:37   \n",
            "724      [Eva Sudholt, www.zeit.de] 2024-10-13 19:59:57   \n",
            "725      [Eva Sudholt, www.zeit.de] 2024-10-13 19:59:57   \n",
            "\n",
            "                                              maintext  \\\n",
            "0    Externer Inhalt von Youtube\\nUm externe Inhalt...   \n",
            "1                                                 None   \n",
            "2    SPD zweifelt an Scholz:\\nMünteferings Mahnung ...   \n",
            "3    F.A.Z. QuarterlyInspirationen und Denkanstöße ...   \n",
            "4    Ampel-Aus:\\nWie wär’s mit etwas mehr Ernsthaft...   \n",
            "..                                                 ...   \n",
            "721  Vor drei Jahren meldete sich Noah Bachofen in ...   \n",
            "722  Meine Suche nach dem besten günstigen Essen vo...   \n",
            "723  Seit vielen Jahren fotografiert der Münchner S...   \n",
            "724  Manchmal meint es das Leben auch gut mit denen...   \n",
            "725  Manchmal meint es das Leben auch gut mit denen...   \n",
            "\n",
            "                                                   url newspaper  \n",
            "0                 https://www.faz.net/aktuell/politik/   faz.net  \n",
            "1    https://www.faz.net/aktuell/politik/antikriegs...   faz.net  \n",
            "2         https://www.faz.net/aktuell/politik/ausland/   faz.net  \n",
            "3    https://www.faz.net/aktuell/politik/briefe-an-...   faz.net  \n",
            "4      https://www.faz.net/aktuell/politik/geschichte/   faz.net  \n",
            "..                                                 ...       ...  \n",
            "721  https://www.zeit.de/zeit-magazin/wochenmarkt/2...   zeit.de  \n",
            "722  https://www.zeit.de/zeit-magazin/wochenmarkt/2...   zeit.de  \n",
            "723  https://www.zeit.de/zeit-magazin/wochenmarkt/2...   zeit.de  \n",
            "724  https://www.zeit.de/zeit-verbrechen/2024/29/lo...   zeit.de  \n",
            "725  https://www.zeit.de/zeit-verbrechen/2024/29/lo...   zeit.de  \n",
            "\n",
            "[726 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rpgmg-LcnhSR",
        "outputId": "e71e9e42-011f-48c6-aa92-b147db698c78"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title             authors  \\\n",
              "0  Politik: Aktuelle Nachrichten aus dem In- und ...  [www.facebook.com]   \n",
              "1  Antikriegs-Marsch: Exil-Russen demonstrieren i...  [www.facebook.com]   \n",
              "2                      Ausland: Aktuelle Nachrichten  [www.facebook.com]   \n",
              "3                          Briefe an die Herausgeber  [www.facebook.com]   \n",
              "4      Geschichte: Historische Themen aus aller Welt  [www.facebook.com]   \n",
              "\n",
              "         date_publish                                           maintext  \\\n",
              "0                 NaT  Externer Inhalt von Youtube\\nUm externe Inhalt...   \n",
              "1 2024-11-17 15:49:56                                               None   \n",
              "2                 NaT  SPD zweifelt an Scholz:\\nMünteferings Mahnung ...   \n",
              "3                 NaT  F.A.Z. QuarterlyInspirationen und Denkanstöße ...   \n",
              "4                 NaT  Ampel-Aus:\\nWie wär’s mit etwas mehr Ernsthaft...   \n",
              "\n",
              "                                                 url newspaper  \n",
              "0               https://www.faz.net/aktuell/politik/   faz.net  \n",
              "1  https://www.faz.net/aktuell/politik/antikriegs...   faz.net  \n",
              "2       https://www.faz.net/aktuell/politik/ausland/   faz.net  \n",
              "3  https://www.faz.net/aktuell/politik/briefe-an-...   faz.net  \n",
              "4    https://www.faz.net/aktuell/politik/geschichte/   faz.net  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-246c5689-dce9-47bb-81b3-56252f8a6d8b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>date_publish</th>\n",
              "      <th>maintext</th>\n",
              "      <th>url</th>\n",
              "      <th>newspaper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Politik: Aktuelle Nachrichten aus dem In- und ...</td>\n",
              "      <td>[www.facebook.com]</td>\n",
              "      <td>NaT</td>\n",
              "      <td>Externer Inhalt von Youtube\\nUm externe Inhalt...</td>\n",
              "      <td>https://www.faz.net/aktuell/politik/</td>\n",
              "      <td>faz.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Antikriegs-Marsch: Exil-Russen demonstrieren i...</td>\n",
              "      <td>[www.facebook.com]</td>\n",
              "      <td>2024-11-17 15:49:56</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.faz.net/aktuell/politik/antikriegs...</td>\n",
              "      <td>faz.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ausland: Aktuelle Nachrichten</td>\n",
              "      <td>[www.facebook.com]</td>\n",
              "      <td>NaT</td>\n",
              "      <td>SPD zweifelt an Scholz:\\nMünteferings Mahnung ...</td>\n",
              "      <td>https://www.faz.net/aktuell/politik/ausland/</td>\n",
              "      <td>faz.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Briefe an die Herausgeber</td>\n",
              "      <td>[www.facebook.com]</td>\n",
              "      <td>NaT</td>\n",
              "      <td>F.A.Z. QuarterlyInspirationen und Denkanstöße ...</td>\n",
              "      <td>https://www.faz.net/aktuell/politik/briefe-an-...</td>\n",
              "      <td>faz.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Geschichte: Historische Themen aus aller Welt</td>\n",
              "      <td>[www.facebook.com]</td>\n",
              "      <td>NaT</td>\n",
              "      <td>Ampel-Aus:\\nWie wär’s mit etwas mehr Ernsthaft...</td>\n",
              "      <td>https://www.faz.net/aktuell/politik/geschichte/</td>\n",
              "      <td>faz.net</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-246c5689-dce9-47bb-81b3-56252f8a6d8b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-246c5689-dce9-47bb-81b3-56252f8a6d8b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-246c5689-dce9-47bb-81b3-56252f8a6d8b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7bb60dfa-618c-4258-b9f3-b1989dce5497\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7bb60dfa-618c-4258-b9f3-b1989dce5497')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7bb60dfa-618c-4258-b9f3-b1989dce5497 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "scraped_data",
              "summary": "{\n  \"name\": \"scraped_data\",\n  \"rows\": 726,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 683,\n        \"samples\": [\n          \"Christian Lindner unter Druck: Noch redet die FDP das Problem herunter\",\n          \"News & Ergebnisse\",\n          \"Axel Schulz sieht klaren Favoriten: Mike Tyson wird Jake Paul \\\"auseinanderschrauben\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date_publish\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-04-14 14:15:00\",\n        \"max\": \"2024-11-17 20:39:52\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"2024-11-14 12:35:10\",\n          \"2024-11-17 13:15:34\",\n          \"2024-11-17 14:46:09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"maintext\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 570,\n        \"samples\": [\n          \"Dieser Text erscheint in einer Reihe namens \\\"Dispatches from LA\\\", die ZEIT ONLINE gemeinsam mit dem Thomas Mann House in Los Angeles gestaltet. Vor und nach der US-Pr\\u00e4sidentschaftswahl am 5. November 2024 berichten aktuelle und ehemalige Fellows des Thomas Mann House f\\u00fcr ZEIT ONLINE \\u00fcber die Gegenwart der USA.\\nDieser Text ist ein Experiment. Die ersten Abs\\u00e4tze sind noch vor der US-Pr\\u00e4sidentschaftswahl geschrieben. In diesem Moment ist noch unklar, wer gewinnt. Aber es zeichnet sich jetzt schon ab, dass es nicht nur auf das Verhalten der Gewinner ankommen wird, sondern auch auf das der Verlierer.\\nDer polnischst\\u00e4mmige Politikwissenschaftler Adam Przeworski hat eine der pr\\u00e4gnantesten Demokratiedefinitionen gefunden: \\\"Democracy is a system in which parties lose elections.\\\" Demokratie ist ein System, in dem Parteien Wahlen verlieren. Er fokussiert damit nicht nur auf die Mehrheit, sondern r\\u00e4umt der Minderheit einen zentralen demokratietheoretischen Platz ein. Entscheidend f\\u00fcr den Fortbestand der Demokratie ist nicht das Verhalten der Gewinner (die freilich gewinnen, aber nicht triumphieren sollen), sondern das Verhalten der Verlierer. Die \\\"good loser norm\\\" besagt, dass der gute Verlierer nicht nur akzeptiert, dass es die M\\u00f6glichkeit einer Niederlage gibt, sondern dass man diese auch eingesteht, sollte man in einem fairen Wettkampf um Stimmen am Ende unterliegen. Der gute Verlierer vertraut darauf, dass er bei der n\\u00e4chsten Wahl wieder antreten und dann eventuell gewinnen kann. Die Akzeptanz des Verlierens ist f\\u00fcr Demokratien zentral.\\nGerade deshalb ist Donald Trump aus demokratietheoretischer Sicht gef\\u00e4hrlich. Bei der Wahl 2016 raunte er noch, dass er die B\\u00fcrger in der Schwebe dar\\u00fcber lassen w\\u00fcrde, ob er eine etwaige Wahlniederlage anerkennen w\\u00fcrde (\\\"keep you in suspense\\\"). Als er dann vier Jahre sp\\u00e4ter tats\\u00e4chlich Joe Biden unterlag und das nicht anerkennen konnte, waren alle Zweifel beseitigt, dass er kein Demokrat im Sinne Przeworskis ist. Bis heute hat er nicht eingestanden, die Wahl von 2020 verloren zu haben; die das Land einende concession speech, bei der man die Niederlage offen einr\\u00e4umt und dem neuen Pr\\u00e4sidenten eine gl\\u00fcckliche Hand w\\u00fcnscht, gab es nicht. Stattdessen den Sturm auf das Kapitol vom 6. Januar 2021.\\nUnd auch in diesem Wahlkampf wurde an der M\\u00e4r von der gestohlenen Wahl festgehalten und Fragen nach dem 6. Januar nicht oder geschichtsverf\\u00e4lschend beantwortet. Die New York Times berichtete zudem \\u00fcber lokale Wahlbeamte, die bei einer sich abzeichnenden Niederlage der Republikaner die Ausz\\u00e4hlung der Stimmen verz\\u00f6gern und somit den Wahlausgang nicht verifizieren wollen. Die Nichtakzeptanz des Verlierens mag Narzissmus, gekr\\u00e4nkter Eitelkeit oder Machtstreben zuzuschreiben sein, die tragische Konsequenz ist f\\u00fcr die US-amerikanische Gesellschaft eine tiefere Spaltung und f\\u00fcr die US-Demokratie ein enormer Stresstest.\\nFalls Harris in ein paar Tagen gewinnen sollte, w\\u00fcrde sich vor allem die Frage stellen, ob Trump und seine Anh\\u00e4nger verlieren k\\u00f6nnen \\u2013 und was deren eventuell sogar gewaltsame Reaktion f\\u00fcr die US-amerikanische Demokratie bedeutet. Sollte die Wahl anders ausgehen, w\\u00e4re die Frage hingegen folgende: Wenn Demokratie ein System ist, in dem man verlieren k\\u00f6nnen muss, was macht es dann mit meinem demokratischen Selbstbewusstsein, sollte Donald Trump gewinnen?\\nGeistig verkatert\\nEs f\\u00e4llt mir schwer, mich auf den zweiten Teil zu konzentrieren; ich wache nach der Wahl in einem anderen Amerika auf, geistig verkatert. Das Frappierende ist die Deutlichkeit des Wahlergebnisses. Aus demokratietheoretischer Sicht ist der Fall hingegen glasklar. Ein guter Demokrat muss eben verlieren k\\u00f6nnen. Das Mandat, das der neue und alte Pr\\u00e4sident Donald Trump vom Volk erhalten hat, ist eindeutig: Er hat den Popular Vote gewonnen, fast in allen Bev\\u00f6lkerungsteilen zugelegt, der Senat ist wieder in republikanischer Hand und das Repr\\u00e4sentantenhaus wahrscheinlich auch. Die Legitimit\\u00e4t der Wahl steht au\\u00dfer Frage. Trump kann mit den auf ihn ausgerichteten Republikanern \\\"durchregieren\\\", und daf\\u00fcr hat er die n\\u00f6tigen Mehrheiten bekommen. Schluss. Aus. Ende der Debatte.\\nDennoch bleibt bei mir eine Beklemmung. Und diese Beklemmung h\\u00e4lt auch noch Tage nach dem 5. November an. Mit der Wahl Trumps blieb der Lackmustest f\\u00fcr die amerikanische Demokratie aus. Trump und seine Anh\\u00e4nger m\\u00fcssen sich nicht zu einer Niederlage verhalten. Stattdessen werden sie von einem Verfahren legitimiert, das sie selbst zu unterminieren gesucht haben.\\nAuf der anderen Seite schien es von Anbeginn unzweifelhaft, dass Kamala Harris den Anstand und die Gr\\u00f6\\u00dfe haben wird, ihre Niederlage \\u00f6ffentlich einzugestehen. Es war vorhersehbar, bei einem Sieg Trumps w\\u00fcrde es keine H\\u00e4ngepartie geben, keine Anfechtungen der Wahlergebnisse, die zentrale Institution der Wahl bliebe unbesch\\u00e4digt. Der schale Beigeschmack, den man nicht wegbekommt, ist der, gegen einen unfairen Trickser verloren zu haben, der im Falle einer Niederlage nicht zum Sieg gratuliert, sondern den Schiedsrichter best\\u00fcrmt h\\u00e4tte.\",\n          \"Herr Dobrindt, was begeistert Sie an einem Wahlkampf im Advent?\\nIn unserem Grundgesetz gibt es klare Regelungen daf\\u00fcr, wie nach einer Vertrauensfrage der Weg zu Neuwahlen ist. Da steht kein Klammersatz mit \\u201eAu\\u00dfer es ist gerade Weihnachten\\u201c. Der Advent ist auch keine politikfreie Zeit und demokratische Wahlen sind nichts Unchristliches. Der Versuch der Rest-Ampel, immer wieder neue Ausreden zu finden, hat echt humoristische Z\\u00fcge angenommen. Man k\\u00f6nne kurz nach Weihnachten nicht w\\u00e4hlen. Man k\\u00f6nne an Karneval nicht w\\u00e4hlen. Man k\\u00f6nne eigentlich \\u00fcberhaupt nicht w\\u00e4hlen, weil es nicht gen\\u00fcgend Papier g\\u00e4be. Kein Wunder, dass viele den Niedergang der Ampel als Trag\\u00f6die empfinden. Dabei kann man hier gar nicht von einer Trag\\u00f6die sprechen.\\nWie meinen Sie das?\\nIn einer Trag\\u00f6die gibt es immer gescheiterte Helden. In der Ampel sehe ich aber keine Helden, nur ganz viel Scheitern. Deswegen war die Ampel viel eher ein Trauerspiel als eine Trag\\u00f6die.\\nEs l\\u00e4sst sich nicht bestreiten, dass Wahlkampf im Winter wenig Spa\\u00df macht. Wie wollen Sie bei den W\\u00e4hlerinnen und W\\u00e4hlern eine optimistische Grundstimmung wecken?\\nIn Bayern haben wir alle sechs Jahre einen Winterwahlkampf. Und zwar bei den Kommunalwahlen, die bei uns traditionell Anfang M\\u00e4rz stattfinden. Dar\\u00fcber hat es noch nie Beschwerden gegeben. Die Sorge um Winterwahlk\\u00e4mpfe ist eine reine Fantasiegeburt derjenigen, die nicht bereit waren, nach dem Scheitern der Ampel schnell Neuwahlen zu erm\\u00f6glichen.\\nWo wird dieser Wahlkampf entschieden?\\nEgal ob im Sommer oder Winter: Wahlk\\u00e4mpfe sind heute sehr stark digital getrieben, sodass ich mir \\u00fcber die Pr\\u00e4senz von politischen Inhalten in dieser Kampagne keine Sorgen mache. Und was den Stra\\u00dfenwahlkampf anbelangt ist der Unterschied zwischen einem Wahlkampfstand im Sommer und einem Wahlkampfstand im Winter doch nur die dicke Jacke.\\nWerden Fake News und Beeinflussung von au\\u00dfen bei dem Wahlkampf eine Rolle spielen?\\nWenn Deutschland w\\u00e4hlt, dann findet das auch Beobachtung bei Kr\\u00e4ften au\\u00dferhalb, die es nicht gut mit uns meinen. Wir werden mit bewussten Fehlinformationen im Netz zu k\\u00e4mpfen haben. Alle Parteien bereiten sich darauf vor, mit Teams in ihren Parteizentralen sehr schnell auf diese falschen Informationen reagieren zu k\\u00f6nnen.\\nSollte es hier eine partei\\u00fcbergreifende Zusammenarbeit geben?\\nIch w\\u00e4re daf\\u00fcr, dass wir uns darauf verst\\u00e4ndigen, Negative Campaigning zu unterlassen. Wir als Union machen das ohnehin nicht, wir wollen die inhaltliche Auseinandersetzung mit unseren politischen Wettbewerbern. Bei anderen Parteien wie der AfD habe ich aber starke Zweifel, dass sie sich an einem fairen Wahlkampf beteiligen wollen. Deren Ma\\u00dfstab ist doch die maximale Polarisierung.\\nSie selbst polarisieren ja auch gern. Werden wir die n\\u00e4chsten zwei Monate vor allem h\\u00f6ren, was alles schlecht gelaufen ist bei der Ampel?\\nIm Rahmen einer demokratischen Auseinandersetzung muss man die politischen Unterschiede deutlich machen. Aber eines ist doch l\\u00e4ngst entschieden, die Menschen in Deutschland wollen einen Politikwechsel. Wir werden im Wahlkampf deswegen sagen, welche Fehlentscheidungen der Ampel wir r\\u00fcckabwickeln. Zum Beispiel das unsinnige Heizungsgesetz, das verkorkste B\\u00fcrgergeld oder die Cannabis-Legalisierung. Zugleich werden wir unsere eigene Zukunftsagenda erkl\\u00e4ren, mit der wir f\\u00fcr Wohlstand, Wachstum und Sicherheit sorgen wollen.\\nWas sind denn die eigenen Zukunftsideen der CSU im Wahlkampf?\\nZun\\u00e4chst muss der wirtschaftliche Abschwung unseres Landes gestoppt werden. Dazu braucht es einen Comeback-Plan f\\u00fcr Deutschland mit wettbewerbsf\\u00e4higen Unternehmenssteuern, einer sicheren und bezahlbaren Energieversorgung sowie einem investitionsfreundlichen Klima, das den Mittelstand nicht aus dem Land treibt. Zweitens m\\u00fcssen wir die illegale Migration in den Griff bekommen und die deutsche Migrationspolitik wieder vom Kopf auf die F\\u00fc\\u00dfe stellen. Und drittens muss Deutschland wieder F\\u00fchrung in Europa \\u00fcbernehmen, um Sicherheit nach innen und au\\u00dfen zu gew\\u00e4hrleisten. Daf\\u00fcr wollen wir Polizei und Bundeswehr st\\u00e4rken.\\nWas glauben Sie denn, welches Thema den Menschen bei der Wahl am wichtigsten sein wird?\\nDie Menschen wollen wieder zuversichtlich sein. Daf\\u00fcr braucht es eine Politik, bei der die Chancen \\u00fcberwiegen und nicht die Risiken. Bei der die Vernunft st\\u00e4rker ist als Ideologie. Und die Aufbruch vermittelt, nicht Stillstand. Eine Politik, die das erm\\u00f6glicht, wird auch deutlich die Polarisierung in unserem Land reduzieren k\\u00f6nnen. Das ist die Grundlage daf\\u00fcr, wieder politisch stabil und mit st\\u00e4rkerer Einigkeit die gro\\u00dfen nationalen und internationalen Herausforderungen bew\\u00e4ltigen zu k\\u00f6nnen.\\nGlauben Sie wirklich, dass Sie mit Migrationspolitik den Menschen Zuversicht vermitteln k\\u00f6nnen?\\nDie illegale Migration ist seit Jahren ein belastendes Thema. Und zwar nicht nur f\\u00fcr die Kommunen, sondern f\\u00fcr die ganze Gesellschaft. Ricarda Lang von den Gr\\u00fcnen hat nach den dramatischen Landtagswahlen in Th\\u00fcringen und Sachsen erkl\\u00e4rt, Migration sei kein Thema. Dieses Verleugnen der Realit\\u00e4t ist die Grundlage f\\u00fcr weitere Polarisierung. Unser Ziel ist es, die Herausforderung bei der illegalen Migration in der politischen Mitte zu l\\u00f6sen. Sonst werden eines Tages radikale Kr\\u00e4fte dieses Thema zu l\\u00f6sen versuchen, die keine R\\u00fccksicht nehmen auf den Rechtsstaat.\\nFriedrich Merz sagt, dass er eigentlich keinen Migrationswahlkampf f\\u00fchren m\\u00f6chte.\\nIch h\\u00e4tte diesen Migrationswahlkampf auch gerne vermieden. Aber ich gehe davon aus, dass dieses Thema bei der anstehenden Bundestagswahl eine bedeutende Rolle spielen wird, gerade weil die Ampel sich in den vergangenen Wochen und Monaten einer gemeinsamen L\\u00f6sung mit der Union verweigert hat. Wir haben den Vorschlag unterbreitet, umfassende Zur\\u00fcckweisungen an den Grenzen vorzunehmen. Das ist mit der Ampel nicht m\\u00f6glich gewesen und f\\u00fchrt dazu, dass wir weiterhin hohe Zahlen illegaler Migration nach Deutschland haben.\\nJens Spahn hat erkl\\u00e4rt, er wolle Minister im n\\u00e4chsten Kabinett werden. W\\u00e4re dann jetzt nicht die Zeit f\\u00fcr einen CSU-Spitzenkandidaten nachzuziehen?\\n(lacht) Selbstverst\\u00e4ndlich unterst\\u00fctze ich die Initiative von Jens Spahn. Jetzt ist die Zeit, alles daf\\u00fcr zu tun, eine maximale St\\u00e4rke der Union bei der Bundestagswahl zu erreichen. Das ist mein einziger Ma\\u00dfstab. Wir brauchen im n\\u00e4chsten Bundestag eine m\\u00f6glichst gro\\u00dfe Unionsfraktion, um dann mit einem m\\u00f6glichst kleinen Koalitionspartner eine Regierung zu bilden. Und zwar eine, die wirklich in der Lage ist, den Politikwechsel umzusetzen.\\nMarkus S\\u00f6der macht bez\\u00fcglich einer zuk\\u00fcnftigen Koalition recht konkrete Vorgaben. Diese Woche haben wir gelernt: Es muss auf jeden Fall die SPD werden.\\nWir f\\u00fchren keine Koalitionsdebatten. Aber wir wollen den W\\u00e4hlern schon eine Orientierung geben, mit wem wir uns vorstellen k\\u00f6nnen, einen Politikwechsel umzusetzen. Die Gr\\u00fcnen haben die wirtschaftliche Misere Deutschlands ma\\u00dfgeblich mitzuverantworten. Sie sind die st\\u00e4ndigen Bremser bei der Reduzierung der illegalen Migration. Und sie haben sich jahrzehntelang dagegen gewehrt, dass eine verteidigungsf\\u00e4hige Bundeswehr mehr Verantwortung in Europa \\u00fcbernimmt. Ich sehe nicht, wie mit einer solchen Partei ein Politikwechsel erreicht werden kann.\\nDies ist die erste Wahl unter neuem Wahlrecht. Wie wird das die Kampagne vor Ort ver\\u00e4ndern?\\nWir werden eine Kampagne umsetzen, die deutlich macht: die Zweitstimme ist Wahlkreisstimme. Wer seinen Direktkandidaten f\\u00fcr die Bundestagswahl unterst\\u00fctzen will, der muss mit der Zweitstimme CSU w\\u00e4hlen. Dieses vollkommen verkorkste und bewusst vermurkste Wahlrecht wird bei der kommenden Wahl der ein oder anderen Ampelpartei selbst auf die F\\u00fc\\u00dfe fallen.\\nKann die FDP bei dieser Wahl mit Hilfe von der Union rechnen?\\nEs gibt keine Leihstimmen egal an welche Partei. Es geht einfach darum, die Union maximal stark zu machen. Nach aktuellen Umfragen wird das Ampel-Wahlrecht dazu f\\u00fchren, dass sich die SPD-Fraktion glatt halbieren wird. Und wenn man schon \\u00fcber Koalitionsm\\u00f6glichkeiten nachdenkt, dann muss man in diesem Fall auch darauf hoffen, dass die vern\\u00fcnftigere H\\u00e4lfte \\u00fcbrigbleibt.\\nWenn die FDP aus dem Bundestag fliegt, verlieren Sie Ihren Traum-Koalitionspartner.\\nDie FDP wird es aus eigener Kraft schaffen m\\u00fcssen, wieder in den Bundestag einzuziehen. Vielleicht gelingt Christian Lindner ja das, was er auch schon mal in der Vergangenheit geschafft hat.\\nAber Sie wollen Christian Lindner keine Zusage f\\u00fcr den Posten des Finanzministers geben?\\nIn der Politik gibt es keine Jobgarantien. Und f\\u00fcr die anstehende Bundestagswahl gilt das in ganz besonderem Ma\\u00dfe.\",\n          \"ATP Finals in Turin\\nHistorischer Doppel-Triumph! Krawietz und P\\u00fctz gewinnen die Tennis-WM\\nDas Warten hat ein Ende! Kevin Krawietz und Tim P\\u00fctz haben als erstes deutsches Duo in der 55-j\\u00e4hrigen Turniergeschichte der ATP Finals das Endspiel im Doppel gewonnen. Die an Nummer eins gesetzten Marcelo Arevalo und Mate Pavic m\\u00fcssen sich in beiden Tiebreaks geschlagen geben. \\u00bb\\n+++ ATP-Finals im Ticker +++\\nZverev verliert Tie-Break-Krimi und fliegt gegen Fritz raus\\nAlexander Zverev scheitert bei den ATP-Finals nach einer herausragenden Vorrunde an Taylor Fritz. Im entscheidenden Satz setzt sich der Amerikaner im Tie-Break durch. Das Spiel zum Nachlesen im Ticker. \\u00bb\\nATP-Finals im Ticker\\nBockstarker Zverev schl\\u00e4gt Alcaraz und steht im Halbfinale\\nWas f\\u00fcr eine Vorrunde von Alexander Zverev! Der deutsche Tennisstar gewinnt gegen Rublev, Ruud und Alcaraz. Damit steht Zverev im Halbfinale der ATP Finals. Das Match zum Nachlesen im Ticker. \\u00bb\\n2. Sieg im 2. Spiel bei ATP Finals\\nStarker Zverev schl\\u00e4gt Ruud in Turin - Alcaraz kann kommen\\nAlexander Zverev setzt sich auch im zweiten Match bei den ATP Finals in Turin souver\\u00e4n durch. Deutschlands Nummer eins gewinnt in zwei S\\u00e4tzen gegen Casper Ruud. Nun geht es im entscheidenden dritten Spiel gegen Carlos Alcaraz um den Einzug ins Halbfinale. \\u00bb\\nAufregung um Taylor Fritz\\nEinbruch in Ferienwohnung von Tennis-Star, Freundin pestet gegen Airbnb\\nTaylor Fritz und seine Freundin Morgan Riddle sind in London beinahe Opfer eines Einbruchs geworden. In ihrer Airbnb-Wohnung wurde eingebrochen. Fritz war drauf und dran, sich mit einem Racket gegen die Eindringlinge zu wehren. \\u00bb\\nTennis-Triumph in Paris\\nZverev dominiert gegen Humbert und gewinnt seinen siebten Masters-Titel\\nNoch ein Titel f\\u00fcr Alex Zverev in diesem Jahr? Der Deutsche trifft im Finale des ATP-Masters-Turniers in Paris-Bercy auf Ugo Humbert. Die Nummer drei der Welt aus Hamburg gegen den Lokalmatadoren, die Nummer 18 aus Frankreich - alle Infos im Liveticker ab 15 Uhr! \\u00bb\\n\\u201eGer\\u00e4t langsam au\\u00dfer Kontrolle\\u201c\\nMitten im Spiel pr\\u00fcgelt Tennis-Star pl\\u00f6tzlich mit dem Schl\\u00e4ger auf sich selbst ein\\nAndrey Rublev hackt mit seinem Schl\\u00e4ger so lange auf seinem Knie rum, dass es blutet. Nicht der erste Ausraster des Tennis-Stars. \\u00bb\\nTennis-Star Nadal vor Abschiedsturnier realistisch\\n\\u201eErwarte schon lange nicht mehr viel vom Ende meiner Karriere\\u201c\\nRafael Nadal w\\u00e4re nicht gl\\u00fccklicher mit 25 Grand-Slam-Titeln, glaubt er - und verr\\u00e4t, warum ein Ende wie im Film gar nicht so erstrebendwert ist. \\u00bb\\nATP-Turnier in Wien\\nErst die Satzf\\u00fchrung, dann das Aus: Zverev scheitert erneut an Musetti\\nAlexander Zverev verliert sein Viertelfinale in Wien gegen Lorenzo Musetti in drei S\\u00e4tzen und scheidet aus dem Turnier aus. Dem Italiener gelingt ein beeindruckendes Comeback und der Einzug ins Halbfinale. Das Tennis-Match zum Nachlesen. \\u00bb\\nBen Shelton\\nMitten im Spiel bekommt Tennis-Star pl\\u00f6tzlich Heiratsantrag von Fan\\nBen Shelton hatte in seinem Erstrundenduell beim ATP 500er-Turnier in Basel gegen Tomas Martin Etcheverry wenig Probleme. Nur der Zwischenruf eines Fans brachte den US-Amerikaner kurz aus dem Konzept. \\u00bb\\nMega-Gewinn f\\u00fcr Medvedev und Rune trotz Blitz-Aus\\nTennis-Stars kassieren f\\u00fcr 70-Minuten-Auftritt in Saudi Arabien 1,5 Millionen\\n1,5 Millionen US-Dollar f\\u00fcr 70 Minuten auf dem Platz. Der Abstecher nach Saudi-Arabien hat sich f\\u00fcr Daniil Medvedev und Holger Rune im wahrsten Sinner des Wortes bezahlt gemacht. \\u00bb\\nTicketpreise explodieren\\nWer Nadals Abschied von der Tennis-B\\u00fchne erleben will, muss tief in die Tasche greifen\\nWer den letzten Auftritt von Rafael Nadal bei den Davis Cup Finals in Malaga live erleben will, ist gezwungen horrende Preise daf\\u00fcr zu bezahlen. \\u00bb\\nKarriereende nach der Saison\\nTennis-Legende Rafael Nadal tritt ab\\nTennis-Superstar Rafael Nadal beendet nach der laufenden Saison seine aktive Karriere. Das teilte der Spanier in einer Videonachricht mit. Der Sandplatzk\\u00f6nig k\\u00e4mpft seit Jahren mit Verletzungsproblemen. \\u00bb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 726,\n        \"samples\": [\n          \"https://www.spiegel.de/sport/tennis/atp-finals-doppel-krawietz-und-puetz-gewinnt-titel-bei-saisonfinale-a-1b82020b-7de4-4c40-8591-05ea7a1a7f27\",\n          \"https://www.welt.de/politik/deutschland/article254558056/Bundeskanzler-Scholz-verteidigt-Telefonat-mit-Putin.html\",\n          \"https://www.welt.de/wirtschaft/plus254491184/Versicherung-Rekordhoch-bei-der-Kfz-Police-So-senken-Sie-Ihre-Beitraege-jetzt-drastisch.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"newspaper\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"focus.de\",\n          \"tagesspiegel.de\",\n          \"faz.net\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMr0yKL8y2Je",
        "outputId": "cacdc13f-3a87-44f4-946f-70465ea33ef8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 726 entries, 0 to 725\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype         \n",
            "---  ------        --------------  -----         \n",
            " 0   title         726 non-null    object        \n",
            " 1   authors       726 non-null    object        \n",
            " 2   date_publish  674 non-null    datetime64[ns]\n",
            " 3   maintext      622 non-null    object        \n",
            " 4   url           726 non-null    object        \n",
            " 5   newspaper     726 non-null    object        \n",
            "dtypes: datetime64[ns](1), object(5)\n",
            "memory usage: 34.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill up NAN-Values in Date_publish and Main Text? Explorative Analysis on dataframe AND trying to scrape por up until around 10k URLS?"
      ],
      "metadata": {
        "id": "4988cgbOy_RV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exkq2I_PzN2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}